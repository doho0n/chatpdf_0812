import os
import streamlit as st
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from PyPDF2 import PdfReader

# 1) PDF â†’ í…ìŠ¤íŠ¸ ë³€í™˜
def extract_text_from_pdf(file):
    pdf = PdfReader(file)
    text = ""
    for page in pdf.pages:
        text += page.extract_text() or ""
    return text

# 2) FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
def build_vectorstore(text):
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = splitter.create_documents([text])
    embeddings = HuggingFaceEmbeddings(model_name="distiluse-base-multilingual-cased-v1")
    vectorstore = FAISS.from_documents(docs, embeddings)
    return vectorstore

# 3) LLM ì´ˆê¸°í™”
@st.cache_resource
def load_openai_llm():
    api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")
    if not api_key:
        st.error("OPENAI_API_KEYë¥¼ í™˜ê²½ë³€ìˆ˜ë‚˜ Streamlit secretsì— ì„¤ì •í•˜ì„¸ìš”.")
        st.stop()
    return ChatOpenAI(model="gpt-4o-mini", temperature=0.0, api_key=api_key)

# 4) Streamlit UI
def main():
    st.set_page_config(page_title="ğŸ“„ ChatPDF", page_icon="ğŸ“„")
    st.title("ğŸ“„ ChatPDF (RAG)")

    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])
    if uploaded_file:
        with st.spinner("PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
            text = extract_text_from_pdf(uploaded_file)
            vectorstore = build_vectorstore(text)
            retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        
        llm = load_openai_llm()

        # ì±„íŒ… ì„¸ì…˜ ì´ˆê¸°í™”
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # ì´ì „ ëŒ€í™” í‘œì‹œ
        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

        # ì‚¬ìš©ì ì…ë ¥
        user_input = st.chat_input("PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")
        if user_input:
            st.chat_message("user").markdown(user_input)
            st.session_state.messages.append({"role": "user", "content": user_input})

            docs = retriever.invoke(user_input)
            if not docs:
                answer = "ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            else:
                context = "\n\n".join([d.page_content for d in docs])
                prompt = f"ë‹¤ìŒ PDF ë‚´ìš©ë§Œ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n\n{context}\n\nì§ˆë¬¸: {user_input}\në‹µë³€:"
                answer = llm.invoke(prompt).content.strip()

            with st.chat_message("assistant"):
                st.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})

if __name__ == "__main__":
    main()
